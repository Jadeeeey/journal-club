## Multi-armed bandit (MAB)
  * https://lilianweng.github.io/lil-log/2018/01/23/the-multi-armed-bandit-problem-and-its-solutions.html
  * http://sanghyukchun.github.io/96/
  * The Contextual Bandits Problem
    * 앞부분의 introduction 파트
    * https://youtu.be/N5x48g2sp8M
    * https://events.csa.iisc.ac.in/LAC15/slides/robert-schapire.pdf
      * Simons talk과 유사하지만 동일한 슬라이드는 아님.

## Application of exploration strategies in MAB to RL
  * Gittins index
  * Upper confidence bound
    * #Exploration:A Study of Count-Based Explorationfor Deep Reinforcement Learning
      * https://arxiv.org/pdf/1611.04717.pdf
  * Expected Improvement
    * Knowledge Gradient
  * Thompson Sampling
    * Deep Exploration via Bootstrapped DQN
      * https://arxiv.org/pdf/1602.04621.pdf

## Bandit + ML
* BanditNet
  * https://openreview.net/pdf?id=SJaP_-xAb
* Hyperband
  * https://arxiv.org/pdf/1603.06560.pdf
  
## 기타
  * Introduction to Multi-Armed Bandits
    * https://arxiv.org/abs/1904.07272
  * 2011 ICML tutorial on bandits
    * https://sites.google.com/site/banditstutorial/
