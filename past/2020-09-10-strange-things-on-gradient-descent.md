### Strange Things on Gradient Descent

#### Does adaptive gradient methods work?
* [Why second order SGD convergence methods are unpopular for deep learning?](https://stats.stackexchange.com/questions/394083/why-second-order-sgd-convergence-methods-are-unpopular-for-deep-learning)
* [The Marginal Value of Adaptive Gradient Methods in Machine Learning](https://arxiv.org/pdf/1705.08292.pdf)
* [Why ADAM Beats SGD for Attention Models](https://arxiv.org/pdf/1912.03194.pdf)

#### Learning rate scheduling
* [Exponential Learning Rate Schedules for Deep Learning](https://arxiv.org/pdf/1910.07454.pdf)
* [Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates](https://arxiv.org/pdf/1708.07120.pdf)
* [Learning with Random Learning Rates](https://arxiv.org/pdf/1810.01322.pdf)
